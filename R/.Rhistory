method = "rf",
metric="Kappa",
trControl=ctrl,
tuneGrid = tune_grid_rf,
tuneLength = 10)
model_rf <- train(trainDat[,predictors],
trainDat[,response],
method = "rf",
metric="Kappa",
trControl=ctrl,
tree=75,
tuneGrid = tune_grid_rf,
tuneLength = 10)
#create a grid for the model with xgbTree
tune_grid <- expand.grid(nrounds=c(100, 200, 300, 400, 500),
max_depth = c(3:7),
eta = c(0.05, 0.5), #eta range
gamma = c(0.01),
colsample_bytree = c(0.75),
subsample = c(0.50),
min_child_weight = c(0))
tune_grid2 <- expand.grid(nrounds=c(100, 200, 300, 400, 500),
max_depth = c(2:8),
eta = c(0.05, 0.5), #eta range
gamma = c(0.01),
colsample_bytree = c(0.85),
subsample = c(0.50),
min_child_weight = c(0))
tune_grid_rf <- expand.grid( mtry = c(4))
#nround: Maximale Anzahl an Iterationen. DefaultWert = 100.
#eta: Learning rate. Je kleiner Eta, desto langsamer wird die Berechnung. Kleineres Eta muss durch mehr Iterationen(nrounds) unterstützt werden. Liegt normalerweise zwischen 0.01 -0.3. Range(0, 1).
#gamma: Kontrolliert die Regularisierung (Auswahl der bevorzugten Komplexität, sodass die   Vorhersagefähigkeit des Modells verbessert wird). Optimales Gamma hängt von dem Datensatz und anderen Parametern ab. Range(0, Inf). default = 0 => keine Regularisierung.
#max_depth: Tiefe des Baums. Desto tiefer der Baum, umso komplizierter wird das Model (Achtung: overfitting)! Größere Datensätze brauchen größere Tiefen, um die Regeln der Daten zu erlernen. Range(0, Inf). default = 6.
#min_child_weight: Auch zur Verhinderung von Overfitting. Range(0, Inf). default = 1.
#subsample: Kontrolliert die Anzahl der Stichproben, die dem Baum übergeben werden.Range(0, 1). Typischerweise zwischen 0.5 - 0.8. default = 1.
#colsample_bytree: Kontrolliert die Anzahl der features die dem Baum übergeben werden. Range(0, 1). Typischerweise zwischen 0.5 - 0.9. default = 1.
#create a grid for the model with xgbTree
tune_grid <- expand.grid(nrounds=c(100, 200, 300, 400, 500),
max_depth = c(3:7),
eta = c(0.05, 0.5), #eta range
gamma = c(0.01),
colsample_bytree = c(0.75),
subsample = c(0.50),
min_child_weight = c(0))
tune_grid2 <- expand.grid(nrounds=c(100, 200, 300, 400, 500),
max_depth = c(2:8),
eta = c(0.05, 0.5), #eta range
gamma = c(0.01),
colsample_bytree = c(0.85),
subsample = c(0.50),
min_child_weight = c(0))
tune_grid_rf <- expand.grid(mtry = c(4))
#nround: Maximale Anzahl an Iterationen. DefaultWert = 100.
#eta: Learning rate. Je kleiner Eta, desto langsamer wird die Berechnung. Kleineres Eta muss durch mehr Iterationen(nrounds) unterstützt werden. Liegt normalerweise zwischen 0.01 -0.3. Range(0, 1).
#gamma: Kontrolliert die Regularisierung (Auswahl der bevorzugten Komplexität, sodass die   Vorhersagefähigkeit des Modells verbessert wird). Optimales Gamma hängt von dem Datensatz und anderen Parametern ab. Range(0, Inf). default = 0 => keine Regularisierung.
#max_depth: Tiefe des Baums. Desto tiefer der Baum, umso komplizierter wird das Model (Achtung: overfitting)! Größere Datensätze brauchen größere Tiefen, um die Regeln der Daten zu erlernen. Range(0, Inf). default = 6.
#min_child_weight: Auch zur Verhinderung von Overfitting. Range(0, Inf). default = 1.
#subsample: Kontrolliert die Anzahl der Stichproben, die dem Baum übergeben werden.Range(0, 1). Typischerweise zwischen 0.5 - 0.8. default = 1.
#colsample_bytree: Kontrolliert die Anzahl der features die dem Baum übergeben werden. Range(0, 1). Typischerweise zwischen 0.5 - 0.9. default = 1.
model_rf <- train(trainDat[,predictors],
trainDat[,response],
method = "rf",
metric="Kappa",
trControl=ctrl,
tree=75,
tuneGrid = tune_grid_rf,
tuneLength = 10)
model_rf <- train(trainDat[,predictors],
trainDat[,response],
method = "rf",
metric="Kappa",
trControl=ctrl,
ntree=75,
tuneGrid = tune_grid_rf,
tuneLength = 10)
model_rf
plot(model_rf)
print(model_rf)
# train the model
set.seed(100)
# model <- train(trainDat[,predictors],
#                trainDat[,response],
#                method="xgbTree",
#                #method = "rf",
#                metric="Kappa",
#                trControl=ctrl,
#                tuneGrid = tune_grid,
#                tuneLength = 10)
#                #nrounds = 100
#                #importance=TRUE,
#                #max_depths=75)
#                #ntree=75)
#
#
# model2 <- train(trainDat[,predictors],
#                trainDat[,response],
#                method="xgbTree",
#                #method = "rf",
#                metric="Kappa",
#                trControl=ctrl,
#                tuneGrid = tune_grid2,
#                tuneLength = 10)
#                #nrounds = 100
#                #importance=TRUE,
#                #max_depths=75)
#                #ntree=75)
model_rf <- train(trainDat[,predictors],
trainDat[,response],
method = "rf",
metric="Kappa",
trControl=ctrl,
tuneGrid = tune_grid_rf,
tuneLength = 10,
ntree=75)
#nrounds = 100
#importance=TRUE,
#max_depths=75)
#ntree=75)
model_rf
plot(model_rf)
# train the model
set.seed(100)
# model <- train(trainDat[,predictors],
#                trainDat[,response],
#                method="xgbTree",
#                #method = "rf",
#                metric="Kappa",
#                trControl=ctrl,
#                tuneGrid = tune_grid,
#                tuneLength = 10)
#                #nrounds = 100
#                #importance=TRUE,
#                #max_depths=75)
#                #ntree=75)
#
#
# model2 <- train(trainDat[,predictors],
#                trainDat[,response],
#                method="xgbTree",
#                #method = "rf",
#                metric="Kappa",
#                trControl=ctrl,
#                tuneGrid = tune_grid2,
#                tuneLength = 10)
#                #nrounds = 100
#                #importance=TRUE,
#                #max_depths=75)
#                #ntree=75)
model_rf <- train(trainDat[,predictors],
trainDat[,response],
method = "rf",
metric="Kappa",
trControl=ctrl,
tuneGrid = tune_grid_rf,
tuneLength = 10,
importance = TRUE,
ntree=75)
plot(model_rf)
# train the model
set.seed(100)
# model <- train(trainDat[,predictors],
#                trainDat[,response],
#                method="xgbTree",
#                #method = "rf",
#                metric="Kappa",
#                trControl=ctrl,
#                tuneGrid = tune_grid,
#                tuneLength = 10)
#                #nrounds = 100
#                #importance=TRUE,
#                #max_depths=75)
#                #ntree=75)
#
#
# model2 <- train(trainDat[,predictors],
#                trainDat[,response],
#                method="xgbTree",
#                #method = "rf",
#                metric="Kappa",
#                trControl=ctrl,
#                tuneGrid = tune_grid2,
#                tuneLength = 10)
#                #nrounds = 100
#                #importance=TRUE,
#                #max_depths=75)
#                #ntree=75)
model_rf <- train(trainDat[,predictors],
trainDat[,response],
method = "rf",
metric="Kappa",
trControl=ctrl,
#tuneGrid = tune_grid_rf,
#tuneLength = 10,
importance = TRUE,
ntree=75)
plot(model_rf)
# train the model
set.seed(100)
# model <- train(trainDat[,predictors],
#                trainDat[,response],
#                method="xgbTree",
#                #method = "rf",
#                metric="Kappa",
#                trControl=ctrl,
#                tuneGrid = tune_grid,
#                tuneLength = 10)
#                #nrounds = 100
#                #importance=TRUE,
#                #max_depths=75)
#                #ntree=75)
#
#
# model2 <- train(trainDat[,predictors],
#                trainDat[,response],
#                method="xgbTree",
#                #method = "rf",
#                metric="Kappa",
#                trControl=ctrl,
#                tuneGrid = tune_grid2,
#                tuneLength = 10)
#                #nrounds = 100
#                #importance=TRUE,
#                #max_depths=75)
#                #ntree=75)
model_rf <- train(trainDat[,predictors],
trainDat[,response],
method = "rf",
metric="Kappa",
trControl=ctrl,
tuneGrid = tune_grid_rf,
tuneLength = 10,
importance = TRUE,
ntree=75)
#print(model)
#print(model2)
plot(varImp(model_rf))
prediction <- predict(sen_ms,model_rf)
cols <- c("sandybrown", "green", "darkred", "blue", "forestgreen", "lightgreen", "red")
tm_shape(deratify(prediction)) +
tm_raster(palette = cols,title = "LUC")+
tm_scale_bar(bg.color="white",bg.alpha=0.75)+
tm_layout(legend.bg.color = "white",
legend.bg.alpha = 0.75)
plot(model_rf)
model_rf
#create a grid for the model with xgbTree
tune_grid <- expand.grid(nrounds=c(100, 200, 300, 400, 500),
max_depth = c(3:7),
eta = c(0.05, 0.5), #eta range
gamma = c(0.01),
colsample_bytree = c(0.75),
subsample = c(0.50),
min_child_weight = c(0))
tune_grid2 <- expand.grid(nrounds=c(100, 200, 300, 400, 500),
max_depth = c(2:8),
eta = c(0.05, 0.5), #eta range
gamma = c(0.01),
colsample_bytree = c(0.85),
subsample = c(0.50),
min_child_weight = c(0))
tune_grid_rf <- expand.grid(mtry = c(4))
tune_grid3 <- expand.grid(nrounds=c(400),
max_depth = c(6),
eta = c(0.05), #eta range
gamma = c(0.01),
colsample_bytree = c(0.75),
subsample = c(0.50),
min_child_weight = c(0))
#nround: Maximale Anzahl an Iterationen. DefaultWert = 100.
#eta: Learning rate. Je kleiner Eta, desto langsamer wird die Berechnung. Kleineres Eta muss durch mehr Iterationen(nrounds) unterstützt werden. Liegt normalerweise zwischen 0.01 -0.3. Range(0, 1).
#gamma: Kontrolliert die Regularisierung (Auswahl der bevorzugten Komplexität, sodass die   Vorhersagefähigkeit des Modells verbessert wird). Optimales Gamma hängt von dem Datensatz und anderen Parametern ab. Range(0, Inf). default = 0 => keine Regularisierung.
#max_depth: Tiefe des Baums. Desto tiefer der Baum, umso komplizierter wird das Model (Achtung: overfitting)! Größere Datensätze brauchen größere Tiefen, um die Regeln der Daten zu erlernen. Range(0, Inf). default = 6.
#min_child_weight: Auch zur Verhinderung von Overfitting. Range(0, Inf). default = 1.
#subsample: Kontrolliert die Anzahl der Stichproben, die dem Baum übergeben werden.Range(0, 1). Typischerweise zwischen 0.5 - 0.8. default = 1.
#colsample_bytree: Kontrolliert die Anzahl der features die dem Baum übergeben werden. Range(0, 1). Typischerweise zwischen 0.5 - 0.9. default = 1.
# train the model
set.seed(100)
model <- train(trainDat[,predictors],
trainDat[,response],
method="xgbTree",
metric="Kappa",
trControl=ctrl,
tuneGrid = tune_grid3,
tuneLength = 10)
#nrounds = 100
#importance=TRUE,
#max_depths=75)
#ntree=75)
#
#
# model2 <- train(trainDat[,predictors],
#                trainDat[,response],
#                method="xgbTree",
#                #method = "rf",
#                metric="Kappa",
#                trControl=ctrl,
#                tuneGrid = tune_grid2,
#                tuneLength = 10)
#                #nrounds = 100
#                #importance=TRUE,
#                #max_depths=75)
#                #ntree=75)
# model_rf <- train(trainDat[,predictors],
#                trainDat[,response],
#                method = "rf",
#                metric="Kappa",
#                trControl=ctrl,
#                tuneGrid = tune_grid_rf,
#                tuneLength = 10,
#                importance = TRUE,
#                ntree=75)
model
#print(model)
#print(model2)
plot(varImp(model))
plot(model)
prediction <- predict(sen_ms,model_rf)
cols <- c("sandybrown", "green", "darkred", "blue", "forestgreen", "lightgreen", "red")
tm_shape(deratify(prediction)) +
tm_raster(palette = cols,title = "LUC")+
tm_scale_bar(bg.color="white",bg.alpha=0.75)+
tm_layout(legend.bg.color = "white",
legend.bg.alpha = 0.75)
model
prediction
knitr::opts_chunk$set(echo = TRUE)
rm(list=ls())
#major required packages:
library(raster)
library(caret)
library(mapview)
library(sf)
#library(devtools)
#install_github("HannaMeyer/CAST")
library(CAST)
#additional required packages:
library(tmap)
library(latticeExtra)
library(doParallel)
library(parallel)
library(Orcs)
sen_ms <- stack("data/Sen_Muenster.grd")
print(sen_ms)
rgbplot_ms <- spplot(sen_ms[[1]],  col.regions="transparent",sp.layout =rgb2spLayout(sen_ms[[1:3]], quantiles = c(0.02, 0.98), alpha = 1))
sen_mr <- stack("data/Sen_Marburg.grd")
rgbplot_mr <- spplot(sen_mr[[1]],  col.regions="transparent",sp.layout =rgb2spLayout(sen_mr[[1:3]], quantiles = c(0.02, 0.98), alpha = 1))
trainSites <- read_sf("data/trainingsites_muenster.gpkg")
print(trainSites)
extr <- extract(sen_ms, trainSites, df=TRUE)
extr <- merge(extr, trainSites, by.x="ID", by.y="PolygonID")
head(extr)
set.seed(100)
trainids <- createDataPartition(extr$ID,list=FALSE,p=0.05)
trainDat <- extr[trainids,]
predictors <- names(sen_ms)
response <- "Label"
indices <- CreateSpacetimeFolds(trainDat,spacevar = "ID",k=3,class="Label")
ctrl <- trainControl(method="cv",
index = indices$index,
savePredictions = TRUE)
#create a grid for the model with xgbTree
tune_grid <- expand.grid(nrounds=c(100, 200, 300, 400, 500),
max_depth = c(3:7),
eta = c(0.05, 0.5), #eta range
gamma = c(0.01),
colsample_bytree = c(0.75),
subsample = c(0.50),
min_child_weight = c(0))
tune_grid2 <- expand.grid(nrounds=c(100, 200, 300, 400, 500),
max_depth = c(2:8),
eta = c(0.05, 0.5), #eta range
gamma = c(0.01),
colsample_bytree = c(0.85),
subsample = c(0.50),
min_child_weight = c(0))
tune_grid_rf <- expand.grid(mtry = c(4))
tune_grid3 <- expand.grid(nrounds=c(400),
max_depth = c(6),
eta = c(0.05), #eta range
gamma = c(0.01),
colsample_bytree = c(0.75),
subsample = c(0.50),
min_child_weight = c(0))
#nround: Maximale Anzahl an Iterationen. DefaultWert = 100.
#eta: Learning rate. Je kleiner Eta, desto langsamer wird die Berechnung. Kleineres Eta muss durch mehr Iterationen(nrounds) unterstützt werden. Liegt normalerweise zwischen 0.01 -0.3. Range(0, 1).
#gamma: Kontrolliert die Regularisierung (Auswahl der bevorzugten Komplexität, sodass die   Vorhersagefähigkeit des Modells verbessert wird). Optimales Gamma hängt von dem Datensatz und anderen Parametern ab. Range(0, Inf). default = 0 => keine Regularisierung.
#max_depth: Tiefe des Baums. Desto tiefer der Baum, umso komplizierter wird das Model (Achtung: overfitting)! Größere Datensätze brauchen größere Tiefen, um die Regeln der Daten zu erlernen. Range(0, Inf). default = 6.
#min_child_weight: Auch zur Verhinderung von Overfitting. Range(0, Inf). default = 1.
#subsample: Kontrolliert die Anzahl der Stichproben, die dem Baum übergeben werden.Range(0, 1). Typischerweise zwischen 0.5 - 0.8. default = 1.
#colsample_bytree: Kontrolliert die Anzahl der features die dem Baum übergeben werden. Range(0, 1). Typischerweise zwischen 0.5 - 0.9. default = 1.
# train the model
set.seed(100)
# model <- train(trainDat[,predictors],
# trainDat[,response],
# method="xgbTree",
# metric="Kappa",
# trControl=ctrl,
# tuneGrid = tune_grid3,
# tuneLength = 10)
#nrounds = 100
#importance=TRUE,
#max_depths=75)
#ntree=75)
#
#
# model2 <- train(trainDat[,predictors],
#                trainDat[,response],
#                method="xgbTree",
#                #method = "rf",
#                metric="Kappa",
#                trControl=ctrl,
#                tuneGrid = tune_grid2,
#                tuneLength = 10)
#                #nrounds = 100
#                #importance=TRUE,
#                #max_depths=75)
#                #ntree=75)
model_rf <- train(trainDat[,predictors],
trainDat[,response],
method = "rf",
metric="Kappa",
trControl=ctrl,
tuneGrid = tune_grid_rf,
tuneLength = 10,
importance = TRUE,
ntree=75)
#print(model)
#print(model2)
plot(varImp(model))
prediction <- predict(sen_ms,model_rf)
cols <- c("sandybrown", "green", "darkred", "blue", "forestgreen", "lightgreen", "red")
tm_shape(deratify(prediction)) +
tm_raster(palette = cols,title = "LUC")+
tm_scale_bar(bg.color="white",bg.alpha=0.75)+
tm_layout(legend.bg.color = "white",
legend.bg.alpha = 0.75)
prediction
plot(prediction)
knitr::opts_chunk$set(echo = TRUE)
sen_ms <- stack("data/Sen_Muenster.grd")
sen_ms <- stack("data/Sen_Muenster.grd")
rm(list=ls())
#major required packages:
library(raster)
library(caret)
library(mapview)
library(sf)
#library(devtools)
#install_github("HannaMeyer/CAST")
library(CAST)
#additional required packages:
library(tmap)
library(latticeExtra)
library(doParallel)
library(parallel)
library(Orcs)
sen_ms <- stack("data/Sen_Muenster.grd")
print(sen_ms)
rgbplot_ms <- spplot(sen_ms[[1]],  col.regions="transparent",sp.layout =rgb2spLayout(sen_ms[[1:3]], quantiles = c(0.02, 0.98), alpha = 1))
sen_mr <- stack("data/Sen_Marburg.grd")
rgbplot_mr <- spplot(sen_mr[[1]],  col.regions="transparent",sp.layout =rgb2spLayout(sen_mr[[1:3]], quantiles = c(0.02, 0.98), alpha = 1))
trainSites <- read_sf("data/trainingsites_muenster.gpkg")
print(trainSites)
viewRGB(sen_ms, r = 3, g = 2, b = 1, map.types = "Esri.WorldImagery")+
mapview(trainSites)
extr <- extract(sen_ms, trainSites, df=TRUE)
extr <- merge(extr, trainSites, by.x="ID", by.y="PolygonID")
head(extr)
extr
trainSites
test c(1, 2, 3)
test <- c(1, 2, 3)
test
remove(test[length(test) - 1])
length(test-1)
x <- length(test-1)
remove(test, 3)
?remove()
remove(3, test)
test <- c(1, 2, 3, "SCL")
test
update.packages()
yes
update.packages()
y
y
update.packages()
library(caret)
library(CAST)
library(Orcs)
library(parallel)
library(rjson)
library(rgeos)
library(rgdal)
library(sf)
library(leafletR)
library(jsonlite)
library(jsonify)
library(latticeExtra)
library(gdalcubes)
library(geojson)
library(tmap)
library(sp)
library(sf)
library(rgeos)
library(rgdal)
setwd("~/")
setwd("~/Documents/Uni/B.Sc. Geoinformatik/Geosoftware 2/workspace/backend/R")
library(lattice)
library(foreach)
library(parallel)
library(iterators)
