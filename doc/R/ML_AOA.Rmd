---
title: "Documentation ML_AOA.R"
author: "Fabian Schumacher"
date: "25 1 2022"
output: html_document
---

```{r setup, include=FALSE}
knitr::opts_chunk$set(echo = TRUE)
```

### Remove all variables from current environment
First of all remove all the variables from the current environment.
```{r remove_all_variables}
rm(list=ls())
```

### Function for model training
**parameters:** \
**algorithm:** (String) Abbreviation for the model type (algorithm) to be used ('rf' or 'svmRadial') \
**trainingDataPath** (String) Relative path of the training data \
**hyperparameter** (Number[]) Array with the hyperparameter for the wanted algorithm \
**desiredBands** (String[]) Array with the desired bands from the sentinel \
**returns:** See return codes
```{r training}
training <- function(algorithm, trainingDataPath, hyperparameter, desiredBands) {

  # load packages
  library(raster)
  library(caret)
  library(CAST)
  library(lattice)
  library(sf)
  library(Orcs)
  library(jsonlite)
  library(kernlab)

  
  # load raster stack from data directory
  stack <- stack("R/processed_sentinel_images/trainingData.tif")
  
  # name the stack bands
  names(stack) <-  desiredBands

  # drop the SCL band
  stack <- dropLayer(stack, length(names(stack)))
  
  # load training data
  trainSites <- read_sf(trainingDataPath)

  # transform training data to crs of the stack 
  trainSites <- st_transform(trainSites, crs = crs(stack))
  
  # add polygon column to the traindata, to merge by ID later on
  trainSites$PolygonID <- 1:nrow(trainSites)
  
  # extract pixels from stack, that are completely covered by the training polygons
  extr_pixel <- extract(stack, trainSites, df=TRUE)
  
  # merge extracted pixels (by "ID"/"PolygonID") with the additional information from the trainsites 
  extr <- merge(extr_pixel, trainSites, by.x="ID", by.y="PolygonID")
  
  # set predictor attributes
  predictors <- names(stack)

  # set response attribute
  response <- "Label"
  
  # create data partition from train data; set percentage of pixels from each polygon will be used (default: 100%)
  trainids <- createDataPartition(extr$ID,list=FALSE,p=1)
  trainDat <- extr[trainids,]

  # make sure that there are no NA-values in the data
  trainDat <- trainDat[complete.cases(trainDat[,predictors]),]  
  
  # create three folds for the spatial-cross-validation which is performed for model training later on
  indices <- CreateSpacetimeFolds(trainDat,spacevar = "ID",k=3,class="Label")
  
  # set trainControl
  ctrl <- trainControl(method="cv", 
                       index = indices$index,
                       savePredictions = TRUE)

  # create tuneGrid with the given hyperparameters 
  if(algorithm == 'rf') {
      tune_grid <- expand.grid( mtry  = c(hyperparameter[1]))
  } else if (algorithm == 'svmRadial'){
      tune_grid <- expand.grid( sigma = c(hyperparameter[1]),
                                C     = c(hyperparameter[2])) 
  }

  # create model through model training
  model <- train(trainDat[,predictors],
                 trainDat[,response],
                 method=algorithm,
                 metric="Kappa",
                 trControl=ctrl,
                 tuneGrid = tune_grid)
  
  # delete files in the folder where the model should be saved into
  files <- list.files(path="./R/model/")
  file.remove(paste("./R/model/",files[1],sep=""))

  # save model as RDS file
  saveRDS(model, file="R/model/model.RDS")

  # return
  return(0)
}
```

### Function to check if the stack has all the predictors the model chosen model uses
**parameters:** \
**stack** (RasterStack) relative filepath of the model to be used \
**model** (Large train) model object that was read from .RDS file \
**returns:** See return codes
```{r checkPredictors}
checkPredictors <- function(model, stack) {
  if (model$method == 'rf') {
    predictors <- model$finalModel$xNames
    bands <- names(stack)
    for (i in 1:length(predictors)) {
      if (!(predictors[i] %in% bands)) {
        return(1)
      }
    }
  } else if (model$method == 'svmRadial') {
    predictors <- names(model$finalModel@scaling$x.scale$`scaled:scale`)
    bands <- names(stack)
    for (i in 1:length(predictors)) {
      if (!(predictors[i] %in% bands)) {
        return(1)
      }
    }
  } else {
    return(4)
  }
}
```


### Function for classifiation and AOA
**parameters:** \
**modelPath** (String) relative filepath of the model to be used \
**desiredBands** (String[]) Array with the desired bands from the sentinel \
**returns:** See return codes
```{r classifyAndAOA}
classifyAndAOA <- function(modelPath, desiredBands) {

  # load packages
  library(raster)
  library(CAST) 
  library(tmap)
  library(latticeExtra)
  library(doParallel)
  library(parallel)
  library(Orcs)
  library(sp)
  library(rgeos)
  library(geojson)
  library(rjson)
  library(kernlab)
  
  # delete old predition.tif and aoa.tif from target directory
  files <- list.files(path="./R/prediction_and_aoa/")
  for (i in 1:length(files)) {
    file.remove(paste("./R/prediction_and_aoa/",files[i],sep=""))
  }

  # load raster stack from data directory
  stack <- stack("R/processed_sentinel_images/aoi.tif")
  names(stack) <- desiredBands
  stack <- dropLayer(stack, length(names(stack)))

  # load model from data directory
  model <- readRDS(modelPath)
  
  # check if all predictors are in the data the predictions are made on
  checkPredictors(model, stack)
  
  # predict the land use with the given model
  prediction <- predict(stack,model)

  # write prediction raster to tif in file directory
  writeRaster(prediction, "R/prediction_and_aoa/prediction.tif", overwrite = TRUE)
  
  # initiate parallel computing
  cl <- makeCluster(6)
  registerDoParallel(cl)

  # calculate AOA for the model prediction
  AOA <- aoa(stack,model,cl=cl)

  # write AOA raster to tif in file directory
  writeRaster(AOA$AOA, "R/prediction_and_aoa/aoa.tif", overwrite=TRUE)
  
  # calculate a MultiPolygon from the AOA, which can be seen as the area where the user needs to find further training data
  x <- AOA$AOA@data@values
  furtherTrainAreas <- rasterToPolygons(AOA$AOA, fun = function(x) {x == 0}, dissolve = TRUE)
  furtherTrainAreas <- spTransform(furtherTrainAreas, CRS("+init=epsg:4326"))
  
  # draw 30 random sample points from MultiPolygon that are suggested as area where the user can look for train data to improve the model
  furtherTrainAreas <- spsample(furtherTrainAreas, n = 30, type = "random")
  
  # delete old furtherTrainAreas.geojson from target directory
  files <- list.files(path="./R/further_train_areas/")
  file.remove(paste("./R/further_train_areas/",files[1],sep=""))
  
  # write furtherTrainAreas as geojson to file directory
  furtherTrainAreasGeoJSON <- as.geojson(furtherTrainAreas)
  geo_write(furtherTrainAreasGeoJSON, "R/further_train_areas/furtherTrainAreas.geojson")

  # save all classes of the prediction to a json file for web usage (create a legend)
  vector <- c()
  if(model$method == 'rf') {
    for(class in model$finalModel$classes) {
      vector <- c(vector, class)
    }
  } else if (model$method == 'svmRadial'){
    for(class in model$finalModel@lev) {
      vector <- c(vector, class)
    }
  }
  json <- rjson::toJSON(vector)
  write(json, "R/prediction_and_aoa/classes.json")
  
  # return
  return(0)
}
```

## Return Codes
The main functions returning codes, which indicating if the creating of a TIF file was successful or not. If it was not successful the code indicates why the program have not run successful. \
**0** - Function successfull \
**1** - Predictor of model is missing in the given Sentinel-Tiff (aoi.tif; only relevant if working with user model) \
**2** - Unexpected Error (return code set in Javascript) \
**3** - Function not executed (return code set in Javascript) \
**4** - Model type is not supported (only relevant if working with user model)


