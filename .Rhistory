}
# else if (algorithm == 'xgbTree') {
#     tune_grid <- expand.grid( nrounds           = c(hyperparameter[1]),
#                               max_depth         = c(hyperparameter[2]),
#                               eta               = c(hyperparameter[3]),
#                               gamma             = c(hyperparameter[4]),
#                               colsample_bytree  = c(hyperparameter[5]),
#                               min_child_weight  = c(hyperparameter[6]),
#                               subsample         = c(hyperparameter[7]))
# }
#Erstellen (Training) des Models
set.seed(100)
model <- train(trainDat[,predictors],
trainDat[,response],
method=algorithm,
metric="Kappa",
trControl=ctrl,
tuneGrid = tune_grid)
#tuneLength = 10) // bin mir nicht sicher welche Auswirkung der Parameter hat
#importance=TRUE,
#ntree=trees)
saveRDS(model, file="R/tempModel/model.RDS")
}
training()
training(algorithm, trainingDataPath, hyperparameter, desiredBands)
# load packages
library(raster)
library(caret)
library(CAST)
library(lattice)
library(sf)
library(Orcs)
library(jsonlite)
# load raster stack from data directory
stack <- stack("R/outputData/trainingData.tif")
names(stack) <-  desiredBands
# load training data
trainSites <- read_sf(trainingDataPath)
trainSites <- st_transform(trainSites, crs = crs(stack))
# Ergänze PolygonID-Spalte falls nicht schon vorhanden, um später mit extrahierten Pixeln zu mergen
trainSites$PolygonID <- 1:nrow(trainSites)
# Extrahiere Pixel aus den Stack, die vollständig vom Polygon abgedeckt werden
extr_pixel <- extract(stack, trainSites, df=TRUE, exact = TRUE)
?extract
install.packages('exactextractr')
library(exactextractr)
# Outputs
#########
# -Trained model as .rds file
setwd("~/Documents/Studium/5. Semester/Geosoftware II/geo-tech-project/backend")
algorithm = 'rf'
trainingDataPath = './public/uploads/trainingsdaten_koeln_4326.gpkg'
hyperparameter = c(2)
desiredBands = c("B02", "B03", "B04", "SCL")
training(algorithm, trainingDataPath, hyperparameter, desiredBands)
training <- function(algorithm, trainingDataPath, hyperparameter, desiredBands) {
# load packages
library(raster)
library(caret)
library(CAST)
library(lattice)
library(sf)
library(Orcs)
library(jsonlite)
library(exactextractr)
# load raster stack from data directory
stack <- stack("R/outputData/trainingData.tif")
names(stack) <-  desiredBands
# load training data
trainSites <- read_sf(trainingDataPath)
trainSites <- st_transform(trainSites, crs = crs(stack))
# Ergänze PolygonID-Spalte falls nicht schon vorhanden, um später mit extrahierten Pixeln zu mergen
trainSites$PolygonID <- 1:nrow(trainSites)
# Extrahiere Pixel aus den Stack, die vollständig vom Polygon abgedeckt werden
extr_pixel <- extract(stack, trainSites, df=TRUE, exact = TRUE)
# Merge extrahierte Pixel mit den zusätzlichen Informationen aus den
extr <- merge(extr_pixel, trainSites, by.x="ID", by.y="PolygonID")
# Prädiktoren und Response festlegen
predictors <- names(stack)
predictors <- predictors[! predictors %in% c('SCL')]
response <- "Label"
# 50% der Pixel eines jeden Polygons für das Modeltraining extrahieren
set.seed(100)
trainids <- createDataPartition(extr$ID,list=FALSE,p=0.5)
trainDat <- extr[trainids,]
trainDat <- trainDat[complete.cases(trainDat[,predictors]),]
# trainIDs <- createDataPartition(extr$ID,p=0.1 , list=FALSE)
# trainIDs
#
# trainData <- extr[trainIDs,]
#
# # Sicherstellen das kein NA in Prädiktoren enthalten ist:
# trainData <- trainData[complete.cases(trainData[,predictors]),]
# trainData
# Drei Folds für die Spatial-Cross-Validation im Modell Training definieren und traincontrol festlegen
indices <- CreateSpacetimeFolds(trainDat,spacevar = "ID",k=3,class="Label")
ctrl <- trainControl(method="cv",
index = indices$index,
savePredictions = TRUE)
#Erstellen eines Grids für die Hyperparameter des jeweiligen Algorithmus:
#hyperparameter <- fromJSON(data)
if(algorithm == 'rf') {
tune_grid <- expand.grid( mtry  = c(hyperparameter[1]))
} else if (algorithm == 'svmRadial'){
tune_grid <- expand.grid( sigma = c(hyperparameter[1]),
C     = c(hyperparameter[2]))
}
# else if (algorithm == 'xgbTree') {
#     tune_grid <- expand.grid( nrounds           = c(hyperparameter[1]),
#                               max_depth         = c(hyperparameter[2]),
#                               eta               = c(hyperparameter[3]),
#                               gamma             = c(hyperparameter[4]),
#                               colsample_bytree  = c(hyperparameter[5]),
#                               min_child_weight  = c(hyperparameter[6]),
#                               subsample         = c(hyperparameter[7]))
# }
#Erstellen (Training) des Models
set.seed(100)
model <- train(trainDat[,predictors],
trainDat[,response],
method=algorithm,
metric="Kappa",
trControl=ctrl,
tuneGrid = tune_grid)
#tuneLength = 10) // bin mir nicht sicher welche Auswirkung der Parameter hat
#importance=TRUE,
#ntree=trees)
saveRDS(model, file="R/tempModel/model.RDS")
}
training(algorithm, trainingDataPath, hyperparameter, desiredBands)
stack()
stack
# load packages
library(raster)
library(caret)
library(CAST)
library(lattice)
library(sf)
library(Orcs)
library(jsonlite)
library(exactextractr)
# load raster stack from data directory
stack <- stack("R/outputData/trainingData.tif")
names(stack) <-  desiredBands
# load training data
trainSites <- read_sf(trainingDataPath)
trainSites <- st_transform(trainSites, crs = crs(stack))
# Ergänze PolygonID-Spalte falls nicht schon vorhanden, um später mit extrahierten Pixeln zu mergen
trainSites$PolygonID <- 1:nrow(trainSites)
# Extrahiere Pixel aus den Stack, die vollständig vom Polygon abgedeckt werden
extr_pixel <- extract(stack, trainSites, df=TRUE, exact = TRUE)
# Extrahiere Pixel aus den Stack, die vollständig (das Zentrum des Pixels wird abgedeckt) vom Polygon abgedeckt werden
extr_pixel <- extract(stack, trainSites, df=TRUE)
# load raster stack from data directory
stack <- stack("R/outputData/trainingData.tif")
names(stack) <-  desiredBands
# load training data
trainSites <- read_sf(trainingDataPath)
trainSites <- st_transform(trainSites, crs = crs(stack))
# Ergänze PolygonID-Spalte falls nicht schon vorhanden, um später mit extrahierten Pixeln zu mergen
trainSites$PolygonID <- 1:nrow(trainSites)
# Extrahiere Pixel aus den Stack, die vollständig (das Zentrum des Pixels wird abgedeckt) vom Polygon abgedeckt werden
extr_pixel <- extract(stack, trainSites, df=TRUE, exact = TRUE)
# Extrahiere Pixel aus den Stack, die vollständig (das Zentrum des Pixels wird abgedeckt) vom Polygon abgedeckt werden
extr_pixel <- extract(stack, trainSites, df=TRUE)
# Outputs
#########
# -Trained model as .rds file
setwd("~/Documents/Studium/5. Semester/Geosoftware II/geo-tech-project/backend")
algorithm = 'rf'
trainingDataPath = './public/uploads/trainingsdaten_koeln_4326.gpkg'
hyperparameter = c(2)
desiredBands = c("B02", "B03", "B04", "SCL")
training <- function(algorithm, trainingDataPath, hyperparameter, desiredBands) {
# load packages
library(raster)
library(caret)
library(CAST)
library(lattice)
library(sf)
library(Orcs)
library(jsonlite)
library(exactextractr)
# load raster stack from data directory
stack <- stack("R/outputData/trainingData.tif")
names(stack) <-  desiredBands
# load training data
trainSites <- read_sf(trainingDataPath)
trainSites <- st_transform(trainSites, crs = crs(stack))
# Ergänze PolygonID-Spalte falls nicht schon vorhanden, um später mit extrahierten Pixeln zu mergen
trainSites$PolygonID <- 1:nrow(trainSites)
# Extrahiere Pixel aus den Stack, die vollständig (das Zentrum des Pixels wird abgedeckt) vom Polygon abgedeckt werden
extr_pixel <- extract(stack, trainSites, df=TRUE)
# Merge extrahierte Pixel mit den zusätzlichen Informationen aus den
extr <- merge(extr_pixel, trainSites, by.x="ID", by.y="PolygonID")
# Prädiktoren und Response festlegen
predictors <- names(stack)
predictors <- predictors[! predictors %in% c('SCL')]
response <- "Label"
# 50% der Pixel eines jeden Polygons für das Modeltraining extrahieren
set.seed(100)
trainids <- createDataPartition(extr$ID,list=FALSE,p=0.5)
trainDat <- extr[trainids,]
trainDat <- trainDat[complete.cases(trainDat[,predictors]),]
# trainIDs <- createDataPartition(extr$ID,p=0.1 , list=FALSE)
# trainIDs
#
# trainData <- extr[trainIDs,]
#
# # Sicherstellen das kein NA in Prädiktoren enthalten ist:
# trainData <- trainData[complete.cases(trainData[,predictors]),]
# trainData
# Drei Folds für die Spatial-Cross-Validation im Modell Training definieren und traincontrol festlegen
indices <- CreateSpacetimeFolds(trainDat,spacevar = "ID",k=3,class="Label")
ctrl <- trainControl(method="cv",
index = indices$index,
savePredictions = TRUE)
#Erstellen eines Grids für die Hyperparameter des jeweiligen Algorithmus:
#hyperparameter <- fromJSON(data)
if(algorithm == 'rf') {
tune_grid <- expand.grid( mtry  = c(hyperparameter[1]))
} else if (algorithm == 'svmRadial'){
tune_grid <- expand.grid( sigma = c(hyperparameter[1]),
C     = c(hyperparameter[2]))
}
# else if (algorithm == 'xgbTree') {
#     tune_grid <- expand.grid( nrounds           = c(hyperparameter[1]),
#                               max_depth         = c(hyperparameter[2]),
#                               eta               = c(hyperparameter[3]),
#                               gamma             = c(hyperparameter[4]),
#                               colsample_bytree  = c(hyperparameter[5]),
#                               min_child_weight  = c(hyperparameter[6]),
#                               subsample         = c(hyperparameter[7]))
# }
#Erstellen (Training) des Models
set.seed(100)
model <- train(trainDat[,predictors],
trainDat[,response],
method=algorithm,
metric="Kappa",
trControl=ctrl,
tuneGrid = tune_grid)
#tuneLength = 10) // bin mir nicht sicher welche Auswirkung der Parameter hat
#importance=TRUE,
#ntree=trees)
saveRDS(model, file="R/tempModel/model.RDS")
}
training(algorithm, trainingDataPath, hyperparameter, desiredBands)
modelPath = "R/tempModel/model.RDS"
classifyAndAOA <- function(modelPath, desiredBands) {
# load packages
library(raster)
library(leafletR)
library(CAST)
library(tmap)
library(latticeExtra)
library(doParallel)
library(parallel)
library(Orcs)
# load raster stack from data directory
stack <- stack("R/outputData/aoi.tif")
names(stack) <- desiredBands
# load raster stack from data directory
model <- readRDS(modelPath)
# prediction
prediction <- predict(stack,model)
# write prediction raster to tif in file directory
writeRaster(prediction, "R/stack/prediction.tif", overwrite = TRUE)
# parallelization
cl <- makeCluster(4)
registerDoParallel(cl)
# calculate AOA
AOA <- aoa(stack,model,cl=cl)
# write prediction raster to tif in file directory
writeRaster(AOA, "R/stack/aoa.tif", overwrite=TRUE)
# Calculate a MultiPolygon from the AOA, which can be seen as the area where the user needs to find further training data
x <- AOA$AOA@data@values
furtherTrainAreas <- rasterToPolygons(AOA$AOA, fun = function(x) {x == 0}, dissolve = TRUE)
# Saves the calculated AOnA to a GeoJSON-file
toGeoJSON(furtherTrainAreas, "furtherTrainAreas", dest = "R/trainAreas", lat.lon, overwrite=TRUE)
}
classifyAndAOA(modelPath)
classifyAndAOA(modelPath, desiredBands)
library(sp)
?spsample
# load packages
library(raster)
library(leafletR)
library(CAST)
library(tmap)
library(latticeExtra)
library(doParallel)
library(parallel)
library(Orcs)
library(sp)
# load raster stack from data directory
stack <- stack("R/outputData/aoi.tif")
names(stack) <- desiredBands
# load raster stack from data directory
model <- readRDS(modelPath)
# prediction
prediction <- predict(stack,model)
# write prediction raster to tif in file directory
writeRaster(prediction, "R/stack/prediction.tif", overwrite = TRUE)
# parallelization
cl <- makeCluster(4)
registerDoParallel(cl)
# calculate AOA
AOA <- aoa(stack,model,cl=cl)
# write prediction raster to tif in file directory
writeRaster(AOA, "R/stack/aoa.tif", overwrite=TRUE)
# Calculate a MultiPolygon from the AOA, which can be seen as the area where the user needs to find further training data
x <- AOA$AOA@data@values
furtherTrainAreas <- rasterToPolygons(AOA$AOA, fun = function(x) {x == 0}, dissolve = TRUE)
spsample(furtherTrainAreas, n = 5, type = "random")
furtherTrainAreas <- spsample(furtherTrainAreas, n = 5, type = "random")
# Saves the calculated AOnA to a GeoJSON-file
toGeoJSON(furtherTrainAreas, "furtherTrainAreas", dest = "R/trainAreas", lat.lon, overwrite=TRUE)
furtherTrainAreas
library(mapview)
mapview(furtherTrainAreas)
plot(furtherTrainAreas)
spplot(furtherTrainAreas)
furtherTrainAreas <- rasterToPolygons(AOA$AOA, fun = function(x) {x == 0}, dissolve = TRUE)
class(furtherTrainAreas)
furtherTrainAreas <- spsample(furtherTrainAreas, n = 5, type = "random")
class(furtherTrainAreas)
furtherTrainAreas <- rasterToPolygons(AOA$AOA, fun = function(x) {x == 0}, dissolve = TRUE)
mapview(furtherTrainAreas)
mapview(stack)
stack
stack
furtherTrainAreas <- spsample(furtherTrainAreas, n = 5, type = "random")
furtherTrainAreas <- rasterToPolygons(AOA$AOA, fun = function(x) {x == 0}, dissolve = TRUE)
furtherTrainAreas <- spsample(furtherTrainAreas, n = 5, type = "random")
# Saves the calculated AOnA to a GeoJSON-file
toGeoJSON(furtherTrainAreas, "furtherTrainAreas", dest = "R/trainAreas", lat.lon, overwrite=TRUE)
?toGeoJSON
furtherTrainAreas
install.packages("geojson")
install.packages("protolite")
install.packages("protolite")
install.packages("protolite")
install.packages("geojson")
install.packages("jqr")
install.packages("jqr")
install.packages("geojson")
# Saves the calculated AOnA to a GeoJSON-file
as.geojson(furtherTrainAreas)
library(geojson)
# Saves the calculated AOnA to a GeoJSON-file
as.geojson(furtherTrainAreas)
geojson <- as.geojson(furtherTrainAreas)
geo_pretty(geojson)
geo_write(geojson, "R/trainAreas/furtherTrainAreas.geojson")
class(geojson)
furtherTrainAreas <- rasterToPolygons(AOA$AOA, fun = function(x) {x == 0}, dissolve = TRUE)
class(furtherTrainAreas)
furtherTrainAreas <- spTransform(furtherTrainAreas, CRS("+init=epsg:4326"))
furtherTrainAreas <- spsample(furtherTrainAreas, n = 5, type = "random")
# Saves the calculated AOnA to a GeoJSON-file
furtherTrainAreasGeoJSON <- as.geojson(furtherTrainAreas)
geo_write(geojson, "R/trainAreas/furtherTrainAreas.geojson")
furtherTrainAreas <- rasterToPolygons(AOA$AOA, fun = function(x) {x == 0}, dissolve = TRUE)
class(furtherTrainAreas)
?spTransform
furtherTrainAreas
crs(furtherTrainAreas)
furtherTrainAreas <- spTransform(furtherTrainAreas, CRS("+proj=longlat +datum=WGS84"))
furtherTrainAreas <- spsample(furtherTrainAreas, n = 5, type = "random")
furtherTrainAreas <- rasterToPolygons(AOA$AOA, fun = function(x) {x == 0}, dissolve = TRUE)
furtherTrainAreas <- spTransform(furtherTrainAreas, CRS("+proj=longlat +datum=WGS84"))
furtherTrainAreas
furtherTrainAreas <- spsample(furtherTrainAreas, n = 5, type = "random")
# Saves the calculated AOnA to a GeoJSON-file
furtherTrainAreasGeoJSON <- as.geojson(furtherTrainAreas)
furtherTrainAreasGeoJSON
geo_pretty(furtherTrainAreasGeoJSON)
crs(furtherTrainAreasGeoJSON)
geo_write(geojson, "R/trainAreas/furtherTrainAreas.geojson")
geo_write(furtherTrainAreasGeoJSON, "R/trainAreas/furtherTrainAreas.geojson")
furtherTrainAreas <- rasterToPolygons(AOA$AOA, fun = function(x) {x == 0}, dissolve = TRUE)
furtherTrainAreas <- spTransform(furtherTrainAreas, CRS("+init=epsg:4326"))
furtherTrainAreas <- spsample(furtherTrainAreas, n = 5, type = "random")
# Saves the calculated AOnA to a GeoJSON-file
furtherTrainAreasGeoJSON <- as.geojson(furtherTrainAreas)
geo_pretty(furtherTrainAreasGeoJSON)
# Outputs
#########
# -Trained model as .rds file
setwd("~/Documents/Studium/5. Semester/Geosoftware II/geo-tech-project/backend")
algorithm = 'rf'
trainingDataPath = './public/uploads/trainingsdaten_koeln_4326.gpkg'
hyperparameter = c(2)
desiredBands = c("B02", "B03", "B04", "SCL")
training <- function(algorithm, trainingDataPath, hyperparameter, desiredBands) {
# load packages
library(raster)
library(caret)
library(CAST)
library(lattice)
library(sf)
library(Orcs)
library(jsonlite)
# load raster stack from data directory
stack <- stack("R/outputData/trainingData.tif")
names(stack) <-  desiredBands
# load training data
trainSites <- read_sf(trainingDataPath)
trainSites <- st_transform(trainSites, crs = crs(stack))
# Ergänze PolygonID-Spalte falls nicht schon vorhanden, um später mit extrahierten Pixeln zu mergen
trainSites$PolygonID <- 1:nrow(trainSites)
# Extrahiere Pixel aus den Stack, die vollständig (das Zentrum des Pixels wird abgedeckt) vom Polygon abgedeckt werden
extr_pixel <- extract(stack, trainSites, df=TRUE)
# Merge extrahierte Pixel mit den zusätzlichen Informationen aus den
extr <- merge(extr_pixel, trainSites, by.x="ID", by.y="PolygonID")
# Prädiktoren und Response festlegen
predictors <- names(stack)
predictors <- predictors[! predictors %in% c('SCL')]
response <- "Label"
# 50% der Pixel eines jeden Polygons für das Modeltraining extrahieren
trainids <- createDataPartition(extr$ID,list=FALSE,p=0.5)
trainDat <- extr[trainids,]
trainDat <- trainDat[complete.cases(trainDat[,predictors]),]
# trainIDs <- createDataPartition(extr$ID,p=0.1 , list=FALSE)
# trainIDs
#
# trainData <- extr[trainIDs,]
#
# # Sicherstellen das kein NA in Prädiktoren enthalten ist:
# trainData <- trainData[complete.cases(trainData[,predictors]),]
# trainData
# Drei Folds für die Spatial-Cross-Validation im Modell Training definieren und traincontrol festlegen
indices <- CreateSpacetimeFolds(trainDat,spacevar = "ID",k=3,class="Label")
ctrl <- trainControl(method="cv",
index = indices$index,
savePredictions = TRUE)
#Erstellen eines Grids für die Hyperparameter des jeweiligen Algorithmus:
#hyperparameter <- fromJSON(data)
if(algorithm == 'rf') {
tune_grid <- expand.grid( mtry  = c(hyperparameter[1]))
} else if (algorithm == 'svmRadial'){
tune_grid <- expand.grid( sigma = c(hyperparameter[1]),
C     = c(hyperparameter[2]))
}
# else if (algorithm == 'xgbTree') {
#     tune_grid <- expand.grid( nrounds           = c(hyperparameter[1]),
#                               max_depth         = c(hyperparameter[2]),
#                               eta               = c(hyperparameter[3]),
#                               gamma             = c(hyperparameter[4]),
#                               colsample_bytree  = c(hyperparameter[5]),
#                               min_child_weight  = c(hyperparameter[6]),
#                               subsample         = c(hyperparameter[7]))
# }
#Erstellen (Training) des Models
model <- train(trainDat[,predictors],
trainDat[,response],
method=algorithm,
metric="Kappa",
trControl=ctrl,
tuneGrid = tune_grid)
#tuneLength = 10) // bin mir nicht sicher welche Auswirkung der Parameter hat
#importance=TRUE,
#ntree=trees)
saveRDS(model, file="R/tempModel/model.RDS")
}
training(algorithm, trainingDataPath, hyperparameter, desiredBands)
modelPath = "R/tempModel/model.RDS"
classifyAndAOA <- function(modelPath, desiredBands) {
# load packages
library(raster)
library(CAST)
library(tmap)
library(latticeExtra)
library(doParallel)
library(parallel)
library(Orcs)
library(sp)
library(geojson)
# load raster stack from data directory
stack <- stack("R/outputData/aoi.tif")
names(stack) <- desiredBands
# load raster stack from data directory
model <- readRDS(modelPath)
# prediction
prediction <- predict(stack,model)
# write prediction raster to tif in file directory
writeRaster(prediction, "R/stack/prediction.tif", overwrite = TRUE)
# parallelization
cl <- makeCluster(4)
registerDoParallel(cl)
# calculate AOA
AOA <- aoa(stack,model,cl=cl)
# write prediction raster to tif in file directory
writeRaster(AOA, "R/stack/aoa.tif", overwrite=TRUE)
# Calculate a MultiPolygon from the AOA, which can be seen as the area where the user needs to find further training data
x <- AOA$AOA@data@values
furtherTrainAreas <- rasterToPolygons(AOA$AOA, fun = function(x) {x == 0}, dissolve = TRUE)
furtherTrainAreas <- spTransform(furtherTrainAreas, CRS("+init=epsg:4326"))
furtherTrainAreas <- spsample(furtherTrainAreas, n = 5, type = "random")
# Saves the calculated AOnA to a GeoJSON-file
furtherTrainAreasGeoJSON <- as.geojson(furtherTrainAreas)
geo_write(furtherTrainAreasGeoJSON, "R/trainAreas/furtherTrainAreas.geojson")
}
classifyAndAOA(modelPath, desiredBands)
# load raster stack from data directory
model <- readRDS(modelPath)
model
length(model$finalModel$classes)
model$finalModel$classes[1]
model$finalModel$classes[2]
model$finalModel$classes[3]
